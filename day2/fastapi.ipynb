{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NGROK_TOKEN'] = 'あなたのNgrokトークン'  # コピーしたトークンをここに貼り付け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# モデル名を変更\n",
    "MODEL_NAME = \"rinna/japanese-gpt-neox-3.6b-instruction-sft\"  # よりアクセスしやすいモデルに変更\n",
    "\n",
    "# モデルのグローバル変数\n",
    "model = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"推論用のLLMモデルを読み込む\"\"\"\n",
    "    global model\n",
    "    try:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"使用デバイス: {device}\")\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=MODEL_NAME,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=device\n",
    "        )\n",
    "        print(f\"モデル '{MODEL_NAME}' の読み込みに成功しました\")\n",
    "        model = pipe\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        print(f\"モデル '{MODEL_NAME}' の読み込みに失敗: {e}\")\n",
    "        return None\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"アプリケーションのライフサイクル管理\"\"\"\n",
    "    # 起動時の処理\n",
    "    load_model()\n",
    "    if model is None:\n",
    "        print(\"警告: 起動時にモデルの初期化に失敗しました\")\n",
    "    else:\n",
    "        print(\"起動時にモデルの初期化が完了しました。\")\n",
    "    yield\n",
    "    # シャットダウン時の処理\n",
    "    print(\"アプリケーションをシャットダウンしています...\")\n",
    "\n",
    "# FastAPIアプリケーション定義\n",
    "app = FastAPI(\n",
    "    title=\"LLM推論API\",\n",
    "    description=\"日本語LLMを使用したテキスト生成のためのAPI\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "# CORSミドルウェアを追加\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# データモデル定義\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_new_tokens: Optional[int] = 512\n",
    "    do_sample: Optional[bool] = True\n",
    "    temperature: Optional[float] = 0.7\n",
    "    top_p: Optional[float] = 0.9\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    generated_text: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"基本的なAPIチェック用のルートエンドポイント\"\"\"\n",
    "    return {\"status\": \"ok\", \"message\": \"LLM API is running\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"ヘルスチェックエンドポイント\"\"\"\n",
    "    global model\n",
    "    if model is None:\n",
    "        return {\"status\": \"error\", \"message\": \"No model loaded\"}\n",
    "    return {\"status\": \"ok\", \"model\": MODEL_NAME}\n",
    "\n",
    "@app.post(\"/generate\", response_model=GenerationResponse)\n",
    "async def generate(request: GenerationRequest):\n",
    "    \"\"\"テキスト生成エンドポイント\"\"\"\n",
    "    global model\n",
    "\n",
    "    if model is None:\n",
    "        print(\"モデルが読み込まれていません。読み込みを試みます...\")\n",
    "        load_model()\n",
    "        if model is None:\n",
    "            raise HTTPException(status_code=503, detail=\"モデルが利用できません。\")\n",
    "\n",
    "    try:\n",
    "        print(f\"リクエストを受信: prompt={request.prompt[:100]}...\")\n",
    "        outputs = model(\n",
    "            request.prompt,\n",
    "            max_new_tokens=request.max_new_tokens,\n",
    "            do_sample=request.do_sample,\n",
    "            temperature=request.temperature,\n",
    "            top_p=request.top_p,\n",
    "        )\n",
    "        \n",
    "        generated_text = outputs[0][\"generated_text\"]\n",
    "        if request.prompt in generated_text:\n",
    "            generated_text = generated_text[len(request.prompt):].strip()\n",
    "        \n",
    "        return GenerationResponse(generated_text=generated_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"応答生成中にエラーが発生しました: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"応答の生成中にエラーが発生しました: {str(e)}\")\n",
    "\n",
    "# ngrokでAPIサーバーを実行\n",
    "def run_with_ngrok(port=8000):\n",
    "    \"\"\"ngrokでFastAPIアプリを実行\"\"\"\n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "    ngrok_token = os.environ.get(\"NGROK_TOKEN\")\n",
    "    if not ngrok_token:\n",
    "        print(\"Ngrok認証トークンが設定されていません。\")\n",
    "        print(\"以下のコマンドを実行してトークンを設定してください：\")\n",
    "        print(\"os.environ['NGROK_TOKEN'] = 'あなたの認証トークン'\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        ngrok.set_auth_token(ngrok_token)\n",
    "        public_url = ngrok.connect(port).public_url\n",
    "        print(f\"ngrok URL: {public_url}\")\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "    except Exception as e:\n",
    "        print(f\"ngrok起動中にエラーが発生しました: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_with_ngrok()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
